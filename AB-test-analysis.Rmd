---
title: "AB-test-analysis"
author: "Daniel Verdejo id=22240224"
date: "2022-11-04"
output: html_document
---

A company is interested in investigating whether adding a new variant to a particular website will improve i) the visit through time and ii) the proportion of query items resolved compared to that observed in the current website. A sample of 300 customers were directed to either the current or new variant of the website and their visit through time whether or not their query was resolved was recorded.
                        
```{r}
library("tidyverse")
library("tolerance")
```

```{r}
data <- data.frame(read.csv("./ab_test.csv", header = TRUE, sep = ","))

set.seed(626) # set the seed for a repeatable results

threshold <- floor(55 * 0.7)

calculateProbability <- function(events, allPossibleOutcomes) {
  return (round(events / allPossibleOutcomes, 3))
}

eventsAboveThresholdExperiment <- function(sample, noOfDays) {
  days <- list()

  for (x in 1:noOfDays) { # for no of days, gather samples of size 55 where resolution status is "Yes"
    day <- slice_sample(sample, n = 55)
    day <- day %>% select(Resolved) %>% filter(Resolved == "Yes")
    day <- count(day)$n
    days <- append(days, day)
  }
  
  # count and return the number of events which meet our condition
  daysMetCondition <- which(days >= threshold)
  noOfEvents <- length(daysMetCondition)
  return (noOfEvents)
}

calcIntervalEstimateTime <- function(variant, variantCnt) {
  sigma <- sqrt(sum((variant$Time - 180)**2) / variantCnt)
  
  stdErrMean <- sigma / (sqrt(variantCnt))
  
  marginOfErr <- qnorm(.95) * stdErrMean
  
  intervalEst <- mean(variant$Time)
  intervalEst <- intervalEst + c(-marginOfErr, marginOfErr)
  return (intervalEst)
}

calcIntervalEstimateResolution <- function(variant, variantCnt) {
  sigma <- (sqrt(sum(variant$ResolvedNumeric)**2) / variantCnt)
  
  stdErrMean <- sigma / (sqrt(variantCnt))
  
  marginOfErr <- qnorm(.95) * stdErrMean
  
  intervalEst <- mean(variantCnt)
  intervalEst <- intervalEst + c(-marginOfErr, marginOfErr)
  return (intervalEst)
}
```

Lets isolate the samples of Variant B our *explanatory variable* and print out the probability that a customer visit time will be below 300 seconds when using this variant.

```{r}
variantB <- data %>%
  select(Variant, Time, Resolved) %>%
  filter(Variant == 'B')

visitsBelow3minVarB <- variantB %>%
  select(Time) %>%
  filter(Time < 180)

variantBCnt <- count(variantB)$n
visitsBelow3minVarBCnt <- count(visitsBelow3minVarB)$n

varBProbabilityVisitBelow3min <- calculateProbability(visitsBelow3minVarBCnt ,variantBCnt)

sprintf("Probability of a visit time below 3 minutes on the new variant (Variant B) = %s / %s = %s", visitsBelow3minVarBCnt, variantBCnt, varBProbabilityVisitBelow3min)
```

To calculate this probability we first gathered a sample where the Variant is "B", this contained 150 data points. Next we gathered a sample of the Variant B sample where the Time value was less than 3 minutes, this contained 49 data points. Using these counts, we calculate the probability by doing the following:


***probability of a visit through time less than 3 minutes = number of Variant B data points where Time is less than 3 minutes / number of Variant B data points*** 



> Our probability that a random customer will have a visit through time below 3 minutes was a score of: **0.327**

---

calc probability that at least 70% of queries on the new site will be resolved in a day where it is assumed that 55
customers visit the site in a day.

Next we will take a sample of 55 customers from the Variant B sample

```{r}
oneDaySample <- sample_n(variantB, 55) %>%
  summarise(count= n(), Time, Resolved, Variant)

ggplot(data = oneDaySample, aes(x = Variant, y = count, fill = Resolved)) +
  geom_bar(position = "fill", stat = "identity") +
  ggtitle("Sample proportion of 55 customers from the Variant B queries")
```
```{r}
noOfDays <- 91 # for a calendar quater of days
eventsAboveThresholdVarB <- eventsAboveThresholdExperiment(variantB, noOfDays)

probabilityResolvedAboveThresholdVarB <- calculateProbability(eventsAboveThresholdVarB, noOfDays)

sprintf("probability that at least 70 percent of queries on the new site will be resolved in a day where it is assumed that 55 customers visit the site = %s", probabilityResolvedAboveThresholdVarB)
```


> Our probability that at least 70% of queries on the new site will be resolved in a day where it is assumed that 55 customers visit the site was a score of: **0.121**

---

Lets compare Variant A on the same probabilities we gathered for Variant B
```{r}
variantA <- data %>%
  select(Variant, Time, Resolved) %>%
  filter(Variant == 'A')

visitsBelow3minVarA <- variantA %>%
  select(Time) %>%
  filter(Time < 180)

variantACnt <- count(variantA)$n
visitsBelow3minVarACnt <- count(visitsBelow3minVarA)$n

varAProbabilityVisitBelow3min <- calculateProbability(visitsBelow3minVarACnt, variantACnt)

sprintf("Probability of a visit time below 3 minutes on the old variant (Variant A) = %s / %s = %s", visitsBelow3minVarACnt, variantACnt, varAProbabilityVisitBelow3min)

eventsAboveThresholdVarA <- eventsAboveThresholdExperiment(variantA, noOfDays)

probabilityResolvedAboveThresholdVarA <- calculateProbability(eventsAboveThresholdVarA, noOfDays)

sprintf("probability that at least 70 percent of queries on the old site will be resolved in a day where it is assumed that 55 customers visit the site = %s", probabilityResolvedAboveThresholdVarA)
```

### The design used

> To gather the probability for both of our experiments the following equation is used:
>
> ***P = # of events which meet our condition / total # of outcomes in our sample*** 

To gather the probability in our experiment: *a visit will be below 3 minutes on the new site* we use a simple enough calculation:

1. We count the number of events which meet our condition: The *Time* value is less than (<) 180 seconds.
2. We count the total number of outcomes: Count all *Time* values found in our sample.
3. We calculate probability by using these like so: P = _visitsBelow180Sec / allVisits_

The same mechanism was used to gather the probability of a visit through time below 3 minutes on the old site - "Variant A".

To gather the probability that *70 percent of queries will be resolved on a day where 55 customers visit the site* we use random sampling of rows for the given variant. 

1. Take 55 randomly selected rows of the variant sample.
2. Count the number of rows where the *Resolved* value was equal to ***"Yes"*** as _positiveResolutionCnt_.
3. Add this count _positiveResolutionCnt_ to a list _positiveResolutionList_.
3. Repeat steps 1, 2, and 3, 50 times.
4. Count the number of events _daysWithResolutionAbove70Percent_ in _positiveResolutionList_ where the list item _i_ was greater than or equal to our threshold (70% of 55 = 38.5 = round down to 38).
5. Calculate the probability by using the following: P = _daysWithResolutionAbove70Percent / totalNumberOfDays_

---

### Concerns with the design used.

The experiment used to gather the probability for the 2nd experiment **70 percent of queries will be resolved on a day where 55 customers visit the site** is not likely to be a true representation of what an average quarter of a year would actually look like in terms of visits per day. It is very likely to have more or less than 55 visits per day but does give a somewhat faithful representation of the probability if you had an average of 55 visits per day. Also the threshold of 70% of 55 worked out to be 38.5 which was rounded down to 38, the data would have differed had we rounded up to 39 (less days would have met the criteria). 

### Numerical and graphical summaries

Now that we have recorded the same probabilities on both variants we can compare the 2 against each other:

```{r}
variants <- unique(sort(data$Variant))
probabilityvisitDurationLessThan3Minutes <- c(varAProbabilityVisitBelow3min, varBProbabilityVisitBelow3min)
probabilityResolvedAbove70percent <- c(probabilityResolvedAboveThresholdVarA, probabilityResolvedAboveThresholdVarB)

comparedProbabilities <- data.frame(variants, probabilityvisitDurationLessThan3Minutes, probabilityResolvedAbove70percent)
comparedProbabilities %>%
  select(variants, probabilityvisitDurationLessThan3Minutes, probabilityResolvedAbove70percent) %>%
  summarise(Variant = variants,
            "Probability of visit less than 3 minutes" = probabilityvisitDurationLessThan3Minutes,
            "Probability of resolved queries above 70% on a random day with 55 visits" = probabilityResolvedAbove70percent)
```

Lets visualise the proportion of resolved to unresolved queries the difference of resolved queries between variants

```{r}
population = count(data)$n

ggplot(data, aes(fill=Resolved, y=population, x=Variant)) +
  geom_bar(position = "fill", stat = "identity") +
  ggtitle("Proportion resolved per variant")
ggplot(data, aes(fill=Time<180, y=population, x=Variant)) +
  geom_bar(position = "fill", stat = "identity") +
  ggtitle("Proportion of time below 180 seconds (3 minutes) per variant")
```
As we can see from stacked bar chart there is a higher proportion of resolved queries on variant B vs variant A. Additionally there is a higher proportion of visits below 180 seconds on variant B vs variant A.

**Comparrison of Time on both variants**

Below we can see from the distribution of times for variant A across the histogram. Our times for each visit fall between ~200 and ~400. This confirms our finding that the probability of a visit being below 180 seconds (3 minutes) is 0.0. We can further confirm this by looking at our minimum, mean and maximum time found in the variant a sample.

The distribution of times differs for variant B. Our time distribution range is significantly lower on the min and max. Additionally we see that there is indeed records below the 180 second thus confirming our finding that the probability of a visit being below 180 seconds (3 minutes) is 0.327. We can further confirm this by looking at our minimum, average and maximum times and comparing them with the same found in the variant A sample.

```{r}
sprintf("Variant A minimum time: %s, average time: %s, maximum time: %s", min(variantA$Time),  mean(variantA$Time),max(variantA$Time))
```
```{r}
sprintf("Variant B minimum time: %s, average time: %s, maximum time: %s", min(variantB$Time),  mean(variantB$Time),max(variantB$Time))
```
As we can see the minimum is as low as ~93 seconds, over 100 seconds faster than our previous minimum and our max time is closer to our previous mean at ~319 seconds (+10%) but ~80 seconds lower than our previous max. The mean time on the new variant is quite close to our 180 second target at 207 seconds further reinforcing the probability found for this metric.


```{r}
hist(variantB$Time, main = "Time distribution", xlab = "duration", prob = TRUE, col = "azure")

lines(density(variantB$Time))
hist(variantA$Time, main = "Time distribution", xlab = "duration", prob = TRUE, col="pink")

lines(density(variantA$Time))
```

### Hypothesis test

Lets test a sample from variant B and gather our confidence interval, sample mean, degrees of freedom and alternative hypothesis

```{r}
t.test(variantB$Time, mu = 180)
variantB <- variantB %>%
  mutate(ResolvedNumeric = ifelse(Resolved == "Yes", 1, 0))
t.test(variantB$ResolvedNumeric, mu = .7)
```
As we can see from our result for **Time** the sample mean is higher at 207.161 than the null hypothesis value of 180. There is convincing evidence as the p-value is very small that the mean is not consistent with the null hypothesis value thus we can reject the null hypothesis.

Likewise for our result for **Resolved** the sample mean is lower than our target of .7. our p-value is also quite small, therefore we can reject the null hypothesis value.

Lets repeat for Variant A

```{r}
t.test(variantA$Time, mu= 180)
variantA <- variantA %>%
  mutate(ResolvedNumeric = ifelse(Resolved == "Yes", 1, 0))
t.test(variantA$ResolvedNumeric, mu = .7)
```
**Time** - We see here our mean is much further away from our target of 180 seconds, we can reject the null hypothesis given our p-value is even smaller than the previous test.


**Resolved** - Again our mean of x is lower than the hypothesis value of .7 and our p-value is minute therefore we can safely reject the null hypthesis value.

### Interval estimation

Below we calculate the interval estimates manually it yields similar results to the tests carried out above for each variant

Variant B Time & Resolution interval estimate
```{r}
calcIntervalEstimateTime(variantB, variantBCnt)
calcIntervalEstimateResolution(variantB, variantBCnt)
```
Variant A Time & Resolution interval estimate
```{r}
calcIntervalEstimateTime(variantA, variantACnt)
calcIntervalEstimateResolution(variantA, variantACnt)

```

### 95/95 Tolerance interval

```{r}
out <- normtol.int(variantB$Time, alpha = 0.05, P = 0.95, side = 1, method = "EXACT")
out
```
Here we see the distribution of our Times for Variant B based on a 2 sided tolerance interval. This describes where 95% of our data lies with 95% confidence.

```{r}
out <- normtol.int(variantA$Time, alpha = 0.05, P = 0.95, side = 1, method = "EXACT")
out
```
Here we see the distribution of our Times for Variant A based on a 2 sided tolerance interval. This describes where 95% of our data lies with 95% confidence.


### Conclusion


We see that from the data gathered and the 2 probabilities we have calculated the new site has:

1. A higher probability that a visit through time will be less than 3 minutes.
2. A higher probability that 70% of queries on the site will be resolved in a day where we assume 55 customers visit the site. 

From these 2 data points alone we could say that new site (Variant B) shows an improvement over the previous variant, but we also found other benefits to the new site including:

1. Lower minimum, average, and maximum visit times
2. A higher proportion of resolved queries ("Yes" responses)

In conclusion given that we see improvements across multiple data points I would feel confident in recommending changing to Variant B.